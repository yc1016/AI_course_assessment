# AI_course_assessment

### Dataset: https://www.kaggle.com/datasets/gpreda/chinese-mnist/data
### Work: Compare the performance of CNN and Transformer

#### 超参数
1. **Epoch** = 10 / 20
2. **Learning Rate** = 0.0002
3. **Batch Size** = 64
4. **8Train Size : Test Size** = 4 : 1 

在单轮训练中，发现 CNN 需要设置较高的 **learning rate=0.001**，而 Transformer 需要设置较低的 **learning rate=0.0001**。为了保证对比实验的严谨性，这里设置两者均为 **0.0002**，效果不错.  
在 5折交叉验证中，会有一折出现未学习到图像特征进行随机猜测的情况，表现为 **loss=2.6-2.7, acc=6%-7%**  

#### CNN (learning rate = 0.001)
| Fold | 平均Train Loss | 平均Train Acc | 平均Val Loss | 平均Val Acc |
|------|----------------|----------------|--------------|--------------|
| 1    | 0.84819        | 74.783%        | 0.56143      | 82.079%      |
| 2    | 0.86031        | 73.231%        | 0.55212      | 82.479%      |
| 3    | 0.94134        | 72.042%        | 0.61852      | 80.833%      |
| 4    | 1.33668        | 58.622%        | 0.96634      | 70.308%      |
| 5    | 0.82337        | 74.766%        | 0.51698      | 82.181%      |
#### ViT (learning rate = 0.0002)
| Fold | 平均Train Loss | 平均Train Acc | 平均Val Loss | 平均Val Acc |
|------|----------------|----------------|--------------|--------------|
| 1    | 1.38677        | 54.736%        | 1.17336      | 64.041%      |
| 2    | 1.27733        | 52.149%        | 1.08332      | 64.077%      |
| 3    | 1.15462        | 57.891%        | 1.01899      | 66.259%      |
| 4    | 1.26613        | 55.162%        | 1.08521      | 64.820%      |
| 5    | 1.28067        | 55.704%        | 1.03204      | 63.926%      |

简单来看，CNN在平均指标上更胜一筹，尽管两者在每折的最终轮次的表现相近

